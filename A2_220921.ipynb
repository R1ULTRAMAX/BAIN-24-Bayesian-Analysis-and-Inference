{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18fz_E4B8VBDLNv6dcxT38Mi4Ah_M9hsK","timestamp":1717310146795},{"file_id":"1dvfQKrKA9lmlWaWuqdhnffl2wxxghYoG","timestamp":1715162137032}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Question 1** : In the folowing definition of a probabilistic model:\n","\n","$Y \\sim \\mathcal{N}(\\mu, \\sigma)$\n","\n","$\\mu \\sim \\mathcal{N}(0, 2)$\n","\n","$\\sigma \\sim \\mathcal{halfnormal}(0.75)$\n","\n","1.1 - Identify the prior and the likelihood.\n","\n","1.2  - How many parameters will the posterior have?\n","\n","1.3 - Compare it with the model for the coin-flipping problem.\n","\n","1.4 - Write Bayes' theorem for the model.\n"],"metadata":{"id":"hwM409w2WnEh"}},{"cell_type":"markdown","source":["# Solution 1.\n","\n","Given the model:\n","\\begin{align*}\n","Y &\\sim N(\\mu, \\sigma) \\\\\n","\\mu &\\sim N(0, 2) \\\\\n","\\sigma &\\sim \\text{halfnormal}(0.75)\n","\\end{align*}\n","\n","**(1.1).**\n","\n","- **Priors**: It is what we know about the parameters (let’s say Ө ). Here, the parameters are ’μ’ and ‘σ’.\n","\n","  So, Prior Distributions are :\n","\n","\\begin{align*}\n","\\mu &\\sim N(0, 2) \\\\\n","\\sigma &\\sim \\text{halfnormal}(0.75)\n","\\end{align*}\n","\n","- **Likelihood**: The likelihood function represents the probability of the data \\(Y\\) given the parameters (’μ’ and ‘σ’). Here, it is given as the normal distribution:\n","\n","\\begin{align*}\n","Y &\\sim N(\\mu, \\sigma) \\\\\n","\\end{align*}\n","\n","**(1.2).**\n","\n","The posterior distribution combines the information from the likelihood and the priors. As mentioned above in (1.1), in this model we have **two** parameters:\n","***’μ’*** and ***‘σ’***.\n","\n","Hence, the posterior distribution will be a joint distribution over these two parameters.\n","\n","**(1.3).**\n","\n","As we have seen in the lecture, the coin-flipping problem typically involves a Beta-Binomial likelihood and a Beta prior. Specifically, if \\(Y\\) is the number of heads in \\(n\\) flips:\n","\n","\\begin{align*}\n","\\text{Likelihood}: & \\quad Y \\mid p \\sim \\text{Binomial}(n, p) \\\\\n","\\text{Prior}: & \\quad p \\sim \\text{Beta}(\\alpha, \\beta)\n","\\end{align*}\n","\n","Here, there is only **one** parameter \\(p\\) which represents the probability of getting head by flipping the coin once.\n","\n","Comparing the two models:\n","\n","1. Dependencies over Prior and Posterior -\n","\n","- This normal model involves **two continuous parameters** ’μ’ and ‘σ’.\n","- The coin-flipping model involves only **one continuous parameter** ‘p’.\n","\n","2. Nature of distribution for Priors and Likelihood -\n","\n","- This model has one prior as Normally distributed and one prior as Half-Normal. Whereas, the likelihood is also a Normal distribution.\n","- The coin-flipping problem typically involves a Beta-Binomial likelihood and a Beta prior.\n","\n","3. Complexity of the Posterior Distribution -\n","\n","- The posterior distribution will be a joint distribution over two parameters, which increases the complexity. The computation of the posterior may become more complex. Visualization is also more challenging as it involves plotting in two dimensions.\n","\n","  Posterior Example:\n","  \n","$$\n","P(\\mu, \\sigma \\mid Y) = \\frac{P(Y \\mid \\mu, \\sigma) P(\\mu) P(\\sigma)}{P(Y)}\n","$$\n","\n","- Since, it is a function of a single parameter. This generally makes the analysis and computation simpler. Analytical solutions may be more feasible, and visualization is straightforward since it involves plotting in one dimension.\n","\n","  Posterior Example:\n","\n","$$\n","P(p \\mid Y) = \\frac{P(Y \\mid p) P(p)}{P(Y)}\n","$$\n","\n","**(1.4).**\n","\n","Bayes' theorem for the given model:\n","\n","$$\n","P(\\mu, \\sigma \\mid Y) = \\frac{P(Y \\mid \\mu, \\sigma) P(\\mu) P(\\sigma)}{P(Y)}\n","$$\n","\n","Let's substitute the given distributions:\n","\n","$$\n","P(\\mu, \\sigma \\mid Y) = \\frac{N(Y \\mid \\mu, \\sigma) \\cdot N(\\mu \\mid 0, 2) \\cdot \\text{halfnormal}(\\sigma \\mid 0.75)}{P(Y)}\n","$$\n","\n","Also,\n","\n","$$\n","P(\\mu, \\sigma \\mid Y) \\propto N(Y \\mid \\mu, \\sigma) \\cdot N(\\mu \\mid 0, 2) \\cdot \\text{halfnormal}(\\sigma \\mid 0.75)\n","$$"],"metadata":{"id":"MPzCtADSrmmy"}},{"cell_type":"code","source":["\"\"\"\n","End of Solution - 1.\n","\"\"\""],"metadata":{"id":"kKjQXLTg1Pd6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question 2**\n","\n","Let’s suppose that we have two coins; when we toss the first coin, half of the time it lands on tails\n"," and half of the time on heads. The other coin is a loaded coin that always lands on heads. If we take\n"," one of the coins at random and get a head, what is the probability that this coin is the unfair one?"],"metadata":{"id":"8IsHvQD6WoGG"}},{"cell_type":"markdown","source":["# Solution 2.\n","\n","Let's define the events involved:\n","\n","- $ H $: Observing a head when a coin is tossed.\n","- $ E_1 $: Choosing a fair coin.\n","- $ E_2 $: Choosing an unfair coin.\n","\n","Given that:\n","\n","1. The probability of getting a head with the fair coin is:\n","\n","   $$ P(H \\mid E_1) = \\frac{1}{2} $$\n","\n","2. The probability of getting a head with the unfair coin is:\n","\n","   $$ P(H \\mid E_2) = 1 $$\n","\n","3. The probability of choosing the fair coin is $ p $, and the probability of choosing the unfair coin is $ (1 - p) $:\n","\n","   $$ P(E_1) = p, \\quad P(E_2) = 1 - p $$\n","\n","We need to find the probability that the coin is the unfair one given that we observed a head:\n","\n","$$ P(E_2 \\mid H) $$\n","\n","According to Bayes' theorem:\n","\n","$$ P(E_2 \\mid H) = \\frac{P(H \\mid E_2) P(E_2)}{P(H)} $$\n","\n","First, we need to calculate $ P(H) $, the total probability of getting a head.\n","\n","As per the law of total probability:\n","\n","$$ P(H) = P(H \\mid E_1)P(E_1) + P(H \\mid E_2)P(E_2) $$\n","\n","It implies:\n","\n","$$ P(H) = \\left( \\frac{1}{2} \\right) p + 1 \\cdot (1 - p) $$\n","\n","$$ P(H) = \\frac{1}{2} p + 1 - p $$\n","\n","$$ P(H) = 1 - \\frac{1}{2} p $$\n","\n","So, substituting it back into Bayes’ theorem, we get:\n","\n","$$ P(E_2 \\mid H) = \\frac{(1) (1 - p)}{1 - \\frac{1}{2} p} $$\n","\n","$$ P(E_2 \\mid H) = \\frac{1 - p}{1 - \\frac{1}{2} p} $$\n","\n","Therefore, the probability that the coin is the unfair one given that we observed a head is:\n","\n","$$ P(E_2 \\mid H) = \\frac{1 - p}{1 - \\frac{1}{2} p} $$\n","\n","If the coins are equally likely to be chosen -\n","\n","Substitute, $ p = \\frac{1}{2} $ into the general formula,\n","\n","Therefore, the probability that the coin is the unfair one given that we observed a head is:\n","$$ P(E_2 \\mid H) = \\frac{2}{3} $$\n"],"metadata":{"id":"0WZlYNDd1i1a"}},{"cell_type":"code","source":["\"\"\"\n","End of Solution - 2.\n","\"\"\""],"metadata":{"id":"SlOZ8Ny3TsNt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question 3**\n","\n","\n","Using PyMC, change the parameters of the prior Beta distribution in `our_first_model` to match those of Week 2. Compare the results."],"metadata":{"id":"VvxJplIpWpaN"}},{"cell_type":"markdown","source":["# Solution 3."],"metadata":{"id":"1rYFzB0V7YQm"}},{"cell_type":"code","source":["# Installing all the requirements.\n","# !pip install pymc==5.8.0 arviz==0.16.1 bambi==0.13.0 pymc-bart==0.5.2 kulprit==0.0.1 preliz==0.3.6 nutpie==0.9.1\n","\n","# Importing all the required libraries.\n","import arviz as az\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import pymc as pm\n","import preliz as pz\n","\n","# Setting up our_first_model.\n","az.style.use(\"arviz-grayscale\")\n","from cycler import cycler\n","default_cycler = cycler(color=[\"#000000\", \"#6a6a6a\", \"#bebebe\", \"#2a2eec\"])\n","plt.rc('axes', prop_cycle=default_cycler)\n","plt.rc('figure', dpi=300)\n","\n","np.random.seed(123)\n","trials = 4\n","theta_real = 0.35 # unknown value in a real experiment\n","\n","# Defining Likelihood.\n","data = pz.Binomial(n = 1, p = theta_real).rvs(trials)\n","\n","# Defining Parameters as per Week-2.\n","\n","alpha_params = [1, 20, 1]\n","beta_params = [1, 20, 4]\n","\n","for i in range(0,3):\n","  with pm.Model() as our_first_model:\n","      θ = pm.Beta('θ', alpha=alpha_params[i], beta=beta_params[i])\n","      y = pm.Bernoulli('y', p=θ, observed=data)\n","      idata = pm.sample(1000, random_seed=4591)\n","      print(\"Parameters : alpha = \", alpha_params[i],\" , beta = \", beta_params[i])\n","      print(az.summary(idata, kind=\"stats\").round(2))\n","      az.plot_trace(idata)\n","      az.plot_trace(idata, kind=\"rank_bars\", combined=True, rank_kwargs={\"colors\": \"k\"});\n","      az.plot_posterior(idata, figsize=(12, 4))\n","      az.plot_bf(idata, var_name=\"θ\", prior=np.random.uniform(0, 1, 10000), ref_val=0.35, figsize=(12, 4), colors=[\"C0\", \"C2\"])\n","      az.plot_posterior(idata, rope=[0.25, .45], figsize=(12, 4))\n","      az.plot_posterior(idata, ref_val=0.35, figsize=(12, 4))"],"metadata":{"id":"RqinTikSWivK","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1-axphC4KEVR97kLINlWUGXhheAxug0U-"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1717570230944,"user_tz":-330,"elapsed":27995,"user":{"displayName":"Purav Jangir","userId":"09178213708020058144"}},"outputId":"7f7d177e-a9a8-47e9-b65a-9c6dd9a42c51"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Please refer to the output of the code for remarks.\n","\n","Following are my observations after looking at the graphs and the values obtained:\n","\n","- Beta($\\alpha$ = 1, $\\beta$ = 1) is a better parameter to follow under posterior distribution as its mean, Savage-Dickey density ratio (ref = 0.35), Region of Practical Equivalence (ROPE = [.25, .45]) are such that, it is more likely to obtain a $\\theta$ = 0.35.\n","\n","  We can rank the beta-priors in order of better results:\n","\n","   Beta($\\alpha$ = 1, $\\beta$ = 1) $>$ Beta($\\alpha$ = 1, $\\beta$ = 4) $>$ Beta($\\alpha$ = 20, $\\beta$ = 20)\n","\n","- In general scenarios, when $\\theta_r$ is unknown and we compare our results for $\\theta_r$ = 0.5, then the above mentioned order will be as follows-\n","\n","   Beta($\\alpha$ = 20, $\\beta$ = 20) $>$ Beta($\\alpha$ = 1, $\\beta$ = 1) $>$ Beta($\\alpha$ = 1, $\\beta$ = 4)\n","\n","- Hence, for stronger prior beliefs we must choose higher values of beta parameters. But they must not be large enough because it may lead to domination of priors in the posterior value obtained. So, small amount of data will not have much impact on the posterior calculation."],"metadata":{"id":"a0utX8jSKv7J"}},{"cell_type":"code","source":["\"\"\"\n","End of Solution - 3.\n","\"\"\""],"metadata":{"id":"iOq0Id4lRbuv"},"execution_count":null,"outputs":[]}]}